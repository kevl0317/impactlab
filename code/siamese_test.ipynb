{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "img_ext = ['jpg', 'png', 'jpeg']\n",
    "\n",
    "segmentation = ['face', 'body', 'clothing', 'environment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1126"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = []\n",
    "for image in os.listdir(f'{data_dir}/images_ps'):\n",
    "    if image.split('.')[1] in img_ext:\n",
    "        image_list.append(image)\n",
    "\n",
    "len(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframe\n",
    "df = pd.read_csv(f'{data_dir}/annotation_data/face/data_faceIdentity.csv').set_index('imageName')\n",
    "\n",
    "columns = []\n",
    "for i in segmentation:\n",
    "    for file in os.listdir(f'{data_dir}/annotation_data/{i}'):\n",
    "        filename = file.split('.')[0]\n",
    "        columns.append(filename)\n",
    "\n",
    "all_features = pd.DataFrame(index=df.index, columns=columns)\n",
    "# all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put data into initialized dataframe and save it\n",
    "for i in segmentation:\n",
    "    for file in os.listdir(f'{data_dir}/annotation_data/{i}'):\n",
    "        filename = file.split('.')[0]\n",
    "        df = pd.read_csv(f'{data_dir}/annotation_data/{i}/{file}').set_index('imageName')\n",
    "        for j in df.index:\n",
    "            all_features.loc[j, filename] = list(df.loc[j])\n",
    "\n",
    "all_features.to_csv('../data/annotation_data/data_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get the aggregated value of the ratings\n",
    "def get_aggregated_value(filename):\n",
    "    trait_name = filename.split('/')[-1].split('_')[0]\n",
    "    df = pd.read_csv(filename).set_index('imageName')\n",
    "    sum = df.sum(axis=1)\n",
    "    count = df.count(axis=1)\n",
    "    df = pd.DataFrame({'sum':sum, 'count':count})\n",
    "    df[f'{trait_name}'] = df['sum'] / df['count']\n",
    "    df = df.get([f'{trait_name}'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goes through the data for all ratings and add the aggregated value for each image to a list\n",
    "traits = []\n",
    "trait_dir = '../data/rawdata_trait_rating/'\n",
    "for trait_csv in os.listdir('../data/rawdata_trait_rating/'):\n",
    "    traits.append(get_aggregated_value(trait_dir + trait_csv))\n",
    "\n",
    "# create dataframe from the aggregated values of each image and then put them into one column training target\n",
    "traits = pd.concat(traits, axis=1)\n",
    "new_column = str(list(traits.columns))\n",
    "traits[new_column] = traits.apply(lambda row: [\n",
    "    row['attractive'], row['competent'], row['dominant'], row['feminine'], row['open'], row['warm'], row['youthful']\n",
    "], axis=1)\n",
    "traits = traits.get([new_column])\n",
    "# traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_faceIdentity</th>\n",
       "      <th>data_gaze</th>\n",
       "      <th>data_action</th>\n",
       "      <th>data_headOrientation</th>\n",
       "      <th>data_jointDistance</th>\n",
       "      <th>data_shape</th>\n",
       "      <th>data_clothColor</th>\n",
       "      <th>data_hairColor</th>\n",
       "      <th>data_environmentColor</th>\n",
       "      <th>data_gist</th>\n",
       "      <th>['attractive', 'competent', 'dominant', 'feminine', 'open', 'warm', 'youthful']</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imageName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>obj365_train_000000121119_000.jpg</th>\n",
       "      <td>[0.12895739, -0.028168134, 0.00757975, -0.0497...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.002333118, 0.009564623, 0.010592721, -0.011...</td>\n",
       "      <td>[7.5218234, -1.1987422, 19.710966]</td>\n",
       "      <td>[0.20098007, 0.3210551, 0.5550481, 0.7843166, ...</td>\n",
       "      <td>[0.5017561, 0.49716136, -0.5002197, 0.5061276,...</td>\n",
       "      <td>[28.738771, 130.58502, 125.413605]</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.5, 3.0]</td>\n",
       "      <td>[144.05965, 129.26022, 131.55663]</td>\n",
       "      <td>[-0.00831051, -0.017807305, -0.007456646, -0.0...</td>\n",
       "      <td>[2.1190476, 4.6666665, 4.2380953, 1.4761904, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COCO_train2014_000000015645.jpg</th>\n",
       "      <td>[0.02622014, 0.081596665, 0.09578178, 0.043360...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.008572297, -0.007843425, 0.012377907, -0.0...</td>\n",
       "      <td>[-45.380337, -14.898459, 3.2193282]</td>\n",
       "      <td>[0.20062017, 0.22529441, 0.40198928, 0.3694324...</td>\n",
       "      <td>[0.5239638, 0.51242507, -0.49369422, 0.5200673...</td>\n",
       "      <td>[183.39679, 140.82408, 139.4729]</td>\n",
       "      <td>[1.25, 2.75, 1.0, 2.0, 1.25]</td>\n",
       "      <td>[89.59149, 155.37447, 159.07156]</td>\n",
       "      <td>[-0.011086742, 0.002311628, 0.00432519, 0.0044...</td>\n",
       "      <td>[4.9761906, 5.452381, 5.2619047, 6.404762, 4.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openimages_9375bad4f7fb7f07_000.jpg</th>\n",
       "      <td>[-0.024200823, 0.033819914, 0.023400877, -0.02...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.007622262, 0.00956685, 0.012877201, -0.010...</td>\n",
       "      <td>[-0.49005643, 18.24955, 10.881761]</td>\n",
       "      <td>[0.20261985, 0.36788186, 0.5587657, 0.5890628,...</td>\n",
       "      <td>[0.50715315, 0.49351236, -0.50082695, 0.504786...</td>\n",
       "      <td>[97.35879, 128.20456, 126.32881]</td>\n",
       "      <td>[1.0, 2.5, 1.0, 2.75, 1.0]</td>\n",
       "      <td>[111.57404, 128.76472, 136.01012]</td>\n",
       "      <td>[-0.008220214, -0.014275488, 0.001812318, -0.0...</td>\n",
       "      <td>[5.418605, 4.604651, 3.1860466, 6.395349, 3.90...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obj365_train_000000152196_000.jpg</th>\n",
       "      <td>[-0.001752781, 0.10471826, 0.09139394, 0.00050...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.022864908, -0.009293311, 0.014308339, -0.0...</td>\n",
       "      <td>[-5.290164, -10.876716, -13.168068]</td>\n",
       "      <td>[0.20239152, 0.29074258, 0.5197746, 0.74333066...</td>\n",
       "      <td>[0.49157432, 0.491557, -0.4998648, 0.5013863, ...</td>\n",
       "      <td>[138.12492, 136.22755, 137.46814]</td>\n",
       "      <td>[1.25, 1.75, 1.0, 3.0, 1.0]</td>\n",
       "      <td>[81.21457, 129.12741, 125.6436]</td>\n",
       "      <td>[0.002911689, -0.017169828, 0.01690869, -0.016...</td>\n",
       "      <td>[5.142857, 4.642857, 3.7380953, 6.6904764, 4.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obj365_train_000000124687_000_v2.jpg</th>\n",
       "      <td>[0.063750975, 0.024633992, 0.018427502, 0.0305...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.016813006, -0.012196642, 0.008504863, -0.0...</td>\n",
       "      <td>[-2.1394632, -7.2794085, 22.632244]</td>\n",
       "      <td>[0.1983144, 0.31450784, 0.5302256, 0.72155076,...</td>\n",
       "      <td>[0.5069542, 0.5005446, -0.48763558, 0.50750524...</td>\n",
       "      <td>[176.008, 136.23224, 121.116394]</td>\n",
       "      <td>[1.5, 1.5, 1.0, 1.5, 2.75]</td>\n",
       "      <td>[127.2692, 141.75041, 136.37422]</td>\n",
       "      <td>[0.003828755, -0.001846732, 0.001435783, -0.02...</td>\n",
       "      <td>[2.139535, 3.6046512, 3.2790697, 1.3023256, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openimages_760d6765c8fd2994_000.jpg</th>\n",
       "      <td>[-0.06154379, -0.013045238, 0.022959623, 0.004...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.019059326, -0.000775224, 0.019176897, -0.0...</td>\n",
       "      <td>[-4.4105678, -10.12891, -0.82248086]</td>\n",
       "      <td>[0.20381983, 0.32736084, 0.56379765, 0.5398477...</td>\n",
       "      <td>[0.5094925, 0.49926317, -0.5049064, 0.5057019,...</td>\n",
       "      <td>[140.48985, 129.8249, 125.32335]</td>\n",
       "      <td>[3.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[118.817726, 133.3955, 133.32318]</td>\n",
       "      <td>[0.01392939, 0.000987544, -0.021520052, -0.036...</td>\n",
       "      <td>[2.4186046, 4.7906976, 3.5581396, 5.4418607, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obj365_train_000000295170_000.jpg</th>\n",
       "      <td>[0.055740464, 0.04205254, -0.025778953, -0.045...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.025736548, -0.01261155, 0.002416412, -0.01...</td>\n",
       "      <td>[-2.0477452, -2.0441082, -10.286333]</td>\n",
       "      <td>[0.20338818, 0.32070142, 0.48857722, 0.5789143...</td>\n",
       "      <td>[0.50178576, 0.50216603, -0.4984893, 0.5135028...</td>\n",
       "      <td>[98.35711, 130.07988, 120.39369]</td>\n",
       "      <td>[1.25, 1.0, 1.0, 1.0, 2.5]</td>\n",
       "      <td>[148.10095, 126.485374, 132.80249]</td>\n",
       "      <td>[0.008089812, 0.011577478, -0.012705069, -0.01...</td>\n",
       "      <td>[2.4651163, 4.581395, 5.023256, 1.3953488, 3.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obj365_train_000000634686_000.jpg</th>\n",
       "      <td>[0.042224776, 0.07374723, 0.10910251, 0.032667...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.01491162, 0.000155397, 0.019540707, -0.024...</td>\n",
       "      <td>[-3.8198817, 3.7038474, -7.004631]</td>\n",
       "      <td>[0.20226246, 0.35610852, 0.5636924, 0.7433996,...</td>\n",
       "      <td>[0.50399476, 0.52361596, -0.49843988, 0.525019...</td>\n",
       "      <td>[170.90103, 128.98155, 116.36889]</td>\n",
       "      <td>[2.75, 1.5, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[100.69099, 118.236824, 142.96867]</td>\n",
       "      <td>[-0.006775786, -0.002211768, 0.018621976, -0.0...</td>\n",
       "      <td>[1.8837209, 4.2619047, 3.3953488, 1.6046512, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COCO_val2014_000000509192.jpg</th>\n",
       "      <td>[-0.060627952, 0.05030816, -0.058176737, -0.04...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.024152804, -0.018798294, 0.009578057, -0.0...</td>\n",
       "      <td>[-3.135318, 6.5448036, -4.636264]</td>\n",
       "      <td>[0.20224802, 0.34307805, 0.5747338, 0.8068926,...</td>\n",
       "      <td>[0.49703178, 0.49528906, -0.50379074, 0.501723...</td>\n",
       "      <td>[44.998253, 131.09077, 124.152214]</td>\n",
       "      <td>[1.75, 2.75, 1.0, 1.0, 1.0]</td>\n",
       "      <td>[164.16908, 130.77658, 126.91156]</td>\n",
       "      <td>[0.02712567, -0.009566761, 0.010354066, -0.018...</td>\n",
       "      <td>[3.511628, 4.5116277, 3.5581396, 6.0465117, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COCO_train2014_000000141543.jpg</th>\n",
       "      <td>[0.002232962, -0.038958598, -0.06340602, 0.025...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[-0.016441824, -0.003458054, 0.008805824, -0.0...</td>\n",
       "      <td>[-4.315872, -9.890037, 0.019214125]</td>\n",
       "      <td>[0.20509605, 0.32835513, 0.5517695, 0.49547014...</td>\n",
       "      <td>[0.5085667, 0.50089216, -0.51217556, 0.4863734...</td>\n",
       "      <td>[51.470345, 132.84949, 130.70059]</td>\n",
       "      <td>[1.0, 3.0, 1.5, 1.0, 1.0]</td>\n",
       "      <td>[119.55985, 134.17955, 148.05103]</td>\n",
       "      <td>[0.028445031, -0.006986384, 0.010896918, -0.00...</td>\n",
       "      <td>[3.5238094, 4.2619047, 3.2142856, 2.3809524, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      data_faceIdentity  \\\n",
       "imageName                                                                                 \n",
       "obj365_train_000000121119_000.jpg     [0.12895739, -0.028168134, 0.00757975, -0.0497...   \n",
       "COCO_train2014_000000015645.jpg       [0.02622014, 0.081596665, 0.09578178, 0.043360...   \n",
       "openimages_9375bad4f7fb7f07_000.jpg   [-0.024200823, 0.033819914, 0.023400877, -0.02...   \n",
       "obj365_train_000000152196_000.jpg     [-0.001752781, 0.10471826, 0.09139394, 0.00050...   \n",
       "obj365_train_000000124687_000_v2.jpg  [0.063750975, 0.024633992, 0.018427502, 0.0305...   \n",
       "...                                                                                 ...   \n",
       "openimages_760d6765c8fd2994_000.jpg   [-0.06154379, -0.013045238, 0.022959623, 0.004...   \n",
       "obj365_train_000000295170_000.jpg     [0.055740464, 0.04205254, -0.025778953, -0.045...   \n",
       "obj365_train_000000634686_000.jpg     [0.042224776, 0.07374723, 0.10910251, 0.032667...   \n",
       "COCO_val2014_000000509192.jpg         [-0.060627952, 0.05030816, -0.058176737, -0.04...   \n",
       "COCO_train2014_000000141543.jpg       [0.002232962, -0.038958598, -0.06340602, 0.025...   \n",
       "\n",
       "                                                           data_gaze  \\\n",
       "imageName                                                              \n",
       "obj365_train_000000121119_000.jpg     [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "COCO_train2014_000000015645.jpg       [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "openimages_9375bad4f7fb7f07_000.jpg   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "obj365_train_000000152196_000.jpg     [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "obj365_train_000000124687_000_v2.jpg  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "...                                                              ...   \n",
       "openimages_760d6765c8fd2994_000.jpg   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "obj365_train_000000295170_000.jpg     [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "obj365_train_000000634686_000.jpg     [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "COCO_val2014_000000509192.jpg         [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "COCO_train2014_000000141543.jpg       [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                                                                            data_action  \\\n",
       "imageName                                                                                 \n",
       "obj365_train_000000121119_000.jpg     [0.002333118, 0.009564623, 0.010592721, -0.011...   \n",
       "COCO_train2014_000000015645.jpg       [-0.008572297, -0.007843425, 0.012377907, -0.0...   \n",
       "openimages_9375bad4f7fb7f07_000.jpg   [-0.007622262, 0.00956685, 0.012877201, -0.010...   \n",
       "obj365_train_000000152196_000.jpg     [-0.022864908, -0.009293311, 0.014308339, -0.0...   \n",
       "obj365_train_000000124687_000_v2.jpg  [-0.016813006, -0.012196642, 0.008504863, -0.0...   \n",
       "...                                                                                 ...   \n",
       "openimages_760d6765c8fd2994_000.jpg   [-0.019059326, -0.000775224, 0.019176897, -0.0...   \n",
       "obj365_train_000000295170_000.jpg     [-0.025736548, -0.01261155, 0.002416412, -0.01...   \n",
       "obj365_train_000000634686_000.jpg     [-0.01491162, 0.000155397, 0.019540707, -0.024...   \n",
       "COCO_val2014_000000509192.jpg         [-0.024152804, -0.018798294, 0.009578057, -0.0...   \n",
       "COCO_train2014_000000141543.jpg       [-0.016441824, -0.003458054, 0.008805824, -0.0...   \n",
       "\n",
       "                                                      data_headOrientation  \\\n",
       "imageName                                                                    \n",
       "obj365_train_000000121119_000.jpg       [7.5218234, -1.1987422, 19.710966]   \n",
       "COCO_train2014_000000015645.jpg        [-45.380337, -14.898459, 3.2193282]   \n",
       "openimages_9375bad4f7fb7f07_000.jpg     [-0.49005643, 18.24955, 10.881761]   \n",
       "obj365_train_000000152196_000.jpg      [-5.290164, -10.876716, -13.168068]   \n",
       "obj365_train_000000124687_000_v2.jpg   [-2.1394632, -7.2794085, 22.632244]   \n",
       "...                                                                    ...   \n",
       "openimages_760d6765c8fd2994_000.jpg   [-4.4105678, -10.12891, -0.82248086]   \n",
       "obj365_train_000000295170_000.jpg     [-2.0477452, -2.0441082, -10.286333]   \n",
       "obj365_train_000000634686_000.jpg       [-3.8198817, 3.7038474, -7.004631]   \n",
       "COCO_val2014_000000509192.jpg            [-3.135318, 6.5448036, -4.636264]   \n",
       "COCO_train2014_000000141543.jpg        [-4.315872, -9.890037, 0.019214125]   \n",
       "\n",
       "                                                                     data_jointDistance  \\\n",
       "imageName                                                                                 \n",
       "obj365_train_000000121119_000.jpg     [0.20098007, 0.3210551, 0.5550481, 0.7843166, ...   \n",
       "COCO_train2014_000000015645.jpg       [0.20062017, 0.22529441, 0.40198928, 0.3694324...   \n",
       "openimages_9375bad4f7fb7f07_000.jpg   [0.20261985, 0.36788186, 0.5587657, 0.5890628,...   \n",
       "obj365_train_000000152196_000.jpg     [0.20239152, 0.29074258, 0.5197746, 0.74333066...   \n",
       "obj365_train_000000124687_000_v2.jpg  [0.1983144, 0.31450784, 0.5302256, 0.72155076,...   \n",
       "...                                                                                 ...   \n",
       "openimages_760d6765c8fd2994_000.jpg   [0.20381983, 0.32736084, 0.56379765, 0.5398477...   \n",
       "obj365_train_000000295170_000.jpg     [0.20338818, 0.32070142, 0.48857722, 0.5789143...   \n",
       "obj365_train_000000634686_000.jpg     [0.20226246, 0.35610852, 0.5636924, 0.7433996,...   \n",
       "COCO_val2014_000000509192.jpg         [0.20224802, 0.34307805, 0.5747338, 0.8068926,...   \n",
       "COCO_train2014_000000141543.jpg       [0.20509605, 0.32835513, 0.5517695, 0.49547014...   \n",
       "\n",
       "                                                                             data_shape  \\\n",
       "imageName                                                                                 \n",
       "obj365_train_000000121119_000.jpg     [0.5017561, 0.49716136, -0.5002197, 0.5061276,...   \n",
       "COCO_train2014_000000015645.jpg       [0.5239638, 0.51242507, -0.49369422, 0.5200673...   \n",
       "openimages_9375bad4f7fb7f07_000.jpg   [0.50715315, 0.49351236, -0.50082695, 0.504786...   \n",
       "obj365_train_000000152196_000.jpg     [0.49157432, 0.491557, -0.4998648, 0.5013863, ...   \n",
       "obj365_train_000000124687_000_v2.jpg  [0.5069542, 0.5005446, -0.48763558, 0.50750524...   \n",
       "...                                                                                 ...   \n",
       "openimages_760d6765c8fd2994_000.jpg   [0.5094925, 0.49926317, -0.5049064, 0.5057019,...   \n",
       "obj365_train_000000295170_000.jpg     [0.50178576, 0.50216603, -0.4984893, 0.5135028...   \n",
       "obj365_train_000000634686_000.jpg     [0.50399476, 0.52361596, -0.49843988, 0.525019...   \n",
       "COCO_val2014_000000509192.jpg         [0.49703178, 0.49528906, -0.50379074, 0.501723...   \n",
       "COCO_train2014_000000141543.jpg       [0.5085667, 0.50089216, -0.51217556, 0.4863734...   \n",
       "\n",
       "                                                         data_clothColor  \\\n",
       "imageName                                                                  \n",
       "obj365_train_000000121119_000.jpg     [28.738771, 130.58502, 125.413605]   \n",
       "COCO_train2014_000000015645.jpg         [183.39679, 140.82408, 139.4729]   \n",
       "openimages_9375bad4f7fb7f07_000.jpg     [97.35879, 128.20456, 126.32881]   \n",
       "obj365_train_000000152196_000.jpg      [138.12492, 136.22755, 137.46814]   \n",
       "obj365_train_000000124687_000_v2.jpg    [176.008, 136.23224, 121.116394]   \n",
       "...                                                                  ...   \n",
       "openimages_760d6765c8fd2994_000.jpg     [140.48985, 129.8249, 125.32335]   \n",
       "obj365_train_000000295170_000.jpg       [98.35711, 130.07988, 120.39369]   \n",
       "obj365_train_000000634686_000.jpg      [170.90103, 128.98155, 116.36889]   \n",
       "COCO_val2014_000000509192.jpg         [44.998253, 131.09077, 124.152214]   \n",
       "COCO_train2014_000000141543.jpg        [51.470345, 132.84949, 130.70059]   \n",
       "\n",
       "                                                    data_hairColor  \\\n",
       "imageName                                                            \n",
       "obj365_train_000000121119_000.jpg        [1.0, 1.0, 1.0, 1.5, 3.0]   \n",
       "COCO_train2014_000000015645.jpg       [1.25, 2.75, 1.0, 2.0, 1.25]   \n",
       "openimages_9375bad4f7fb7f07_000.jpg     [1.0, 2.5, 1.0, 2.75, 1.0]   \n",
       "obj365_train_000000152196_000.jpg      [1.25, 1.75, 1.0, 3.0, 1.0]   \n",
       "obj365_train_000000124687_000_v2.jpg    [1.5, 1.5, 1.0, 1.5, 2.75]   \n",
       "...                                                            ...   \n",
       "openimages_760d6765c8fd2994_000.jpg      [3.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "obj365_train_000000295170_000.jpg       [1.25, 1.0, 1.0, 1.0, 2.5]   \n",
       "obj365_train_000000634686_000.jpg       [2.75, 1.5, 1.0, 1.0, 1.0]   \n",
       "COCO_val2014_000000509192.jpg          [1.75, 2.75, 1.0, 1.0, 1.0]   \n",
       "COCO_train2014_000000141543.jpg          [1.0, 3.0, 1.5, 1.0, 1.0]   \n",
       "\n",
       "                                                   data_environmentColor  \\\n",
       "imageName                                                                  \n",
       "obj365_train_000000121119_000.jpg      [144.05965, 129.26022, 131.55663]   \n",
       "COCO_train2014_000000015645.jpg         [89.59149, 155.37447, 159.07156]   \n",
       "openimages_9375bad4f7fb7f07_000.jpg    [111.57404, 128.76472, 136.01012]   \n",
       "obj365_train_000000152196_000.jpg        [81.21457, 129.12741, 125.6436]   \n",
       "obj365_train_000000124687_000_v2.jpg    [127.2692, 141.75041, 136.37422]   \n",
       "...                                                                  ...   \n",
       "openimages_760d6765c8fd2994_000.jpg    [118.817726, 133.3955, 133.32318]   \n",
       "obj365_train_000000295170_000.jpg     [148.10095, 126.485374, 132.80249]   \n",
       "obj365_train_000000634686_000.jpg     [100.69099, 118.236824, 142.96867]   \n",
       "COCO_val2014_000000509192.jpg          [164.16908, 130.77658, 126.91156]   \n",
       "COCO_train2014_000000141543.jpg        [119.55985, 134.17955, 148.05103]   \n",
       "\n",
       "                                                                              data_gist  \\\n",
       "imageName                                                                                 \n",
       "obj365_train_000000121119_000.jpg     [-0.00831051, -0.017807305, -0.007456646, -0.0...   \n",
       "COCO_train2014_000000015645.jpg       [-0.011086742, 0.002311628, 0.00432519, 0.0044...   \n",
       "openimages_9375bad4f7fb7f07_000.jpg   [-0.008220214, -0.014275488, 0.001812318, -0.0...   \n",
       "obj365_train_000000152196_000.jpg     [0.002911689, -0.017169828, 0.01690869, -0.016...   \n",
       "obj365_train_000000124687_000_v2.jpg  [0.003828755, -0.001846732, 0.001435783, -0.02...   \n",
       "...                                                                                 ...   \n",
       "openimages_760d6765c8fd2994_000.jpg   [0.01392939, 0.000987544, -0.021520052, -0.036...   \n",
       "obj365_train_000000295170_000.jpg     [0.008089812, 0.011577478, -0.012705069, -0.01...   \n",
       "obj365_train_000000634686_000.jpg     [-0.006775786, -0.002211768, 0.018621976, -0.0...   \n",
       "COCO_val2014_000000509192.jpg         [0.02712567, -0.009566761, 0.010354066, -0.018...   \n",
       "COCO_train2014_000000141543.jpg       [0.028445031, -0.006986384, 0.010896918, -0.00...   \n",
       "\n",
       "                                     ['attractive', 'competent', 'dominant', 'feminine', 'open', 'warm', 'youthful']  \n",
       "imageName                                                                                                             \n",
       "obj365_train_000000121119_000.jpg     [2.1190476, 4.6666665, 4.2380953, 1.4761904, 3...                               \n",
       "COCO_train2014_000000015645.jpg       [4.9761906, 5.452381, 5.2619047, 6.404762, 4.7...                               \n",
       "openimages_9375bad4f7fb7f07_000.jpg   [5.418605, 4.604651, 3.1860466, 6.395349, 3.90...                               \n",
       "obj365_train_000000152196_000.jpg     [5.142857, 4.642857, 3.7380953, 6.6904764, 4.0...                               \n",
       "obj365_train_000000124687_000_v2.jpg  [2.139535, 3.6046512, 3.2790697, 1.3023256, 3....                               \n",
       "...                                                                                 ...                               \n",
       "openimages_760d6765c8fd2994_000.jpg   [2.4186046, 4.7906976, 3.5581396, 5.4418607, 3...                               \n",
       "obj365_train_000000295170_000.jpg     [2.4651163, 4.581395, 5.023256, 1.3953488, 3.2...                               \n",
       "obj365_train_000000634686_000.jpg     [1.8837209, 4.2619047, 3.3953488, 1.6046512, 4...                               \n",
       "COCO_val2014_000000509192.jpg         [3.511628, 4.5116277, 3.5581396, 6.0465117, 3....                               \n",
       "COCO_train2014_000000141543.jpg       [3.5238094, 4.2619047, 3.2142856, 2.3809524, 4...                               \n",
       "\n",
       "[1125 rows x 11 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the training data and the training target\n",
    "all_data = all_features.merge(traits, left_index=True, right_index=True)\n",
    "\n",
    "# Define a function to convert list elements to floats\n",
    "def convert_list_elements_to_float(cell):\n",
    "    if isinstance(cell, list):\n",
    "        return np.array(cell, dtype=np.float32)\n",
    "    return cell\n",
    "\n",
    "# Apply the function to each cell in the DataFrame\n",
    "all_data = all_data.map(convert_list_elements_to_float)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# seperate the training data and the target label\n",
    "X = all_data.get(list(all_data.columns)[:-1])\n",
    "\n",
    "# dataframe to store scaled data\n",
    "scaled_X = pd.DataFrame(index=X.index)\n",
    "\n",
    "# apply Min-Max scaling to training data\n",
    "for i, feature in enumerate(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    stacked = np.vstack(X[feature])\n",
    "    scaled = scaler.fit_transform(stacked)\n",
    "    unstacked = [list(row) for row in scaled]\n",
    "    scaled_X[feature] = unstacked\n",
    "\n",
    "# check index sequence\n",
    "assert(scaled_X.index.all()==y.index.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need to create labels (similar/dissmilar) for data pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    a = np.array(a).reshape(1, -1)\n",
    "    b = np.array(b).reshape(1, -1)\n",
    "    return cosine_similarity(a, b)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageName</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obj365_train_000000121119_000.jpg</td>\n",
       "      <td>[0.92500126, 0.3313901, 0.48801482, 0.3691098,...</td>\n",
       "      <td>[2.1190476, 4.6666665, 4.2380953, 1.4761904, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000015645.jpg</td>\n",
       "      <td>[0.5208456, 0.74441016, 0.82499117, 0.7367672,...</td>\n",
       "      <td>[4.9761906, 5.452381, 5.2619047, 6.404762, 4.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openimages_9375bad4f7fb7f07_000.jpg</td>\n",
       "      <td>[0.3224957, 0.56463706, 0.54845953, 0.4655692,...</td>\n",
       "      <td>[5.418605, 4.604651, 3.1860466, 6.395349, 3.90...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obj365_train_000000152196_000.jpg</td>\n",
       "      <td>[0.41080356, 0.8314114, 0.8082274, 0.56753117,...</td>\n",
       "      <td>[5.142857, 4.642857, 3.7380953, 6.6904764, 4.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obj365_train_000000124687_000_v2.jpg</td>\n",
       "      <td>[0.66848725, 0.5300725, 0.5294587, 0.68622065,...</td>\n",
       "      <td>[2.139535, 3.6046512, 3.2790697, 1.3023256, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>openimages_760d6765c8fd2994_000.jpg</td>\n",
       "      <td>[0.17559308, 0.38829413, 0.54677373, 0.5846894...</td>\n",
       "      <td>[2.4186046, 4.7906976, 3.5581396, 5.4418607, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>obj365_train_000000295170_000.jpg</td>\n",
       "      <td>[0.63697493, 0.59561455, 0.36056772, 0.3848691...</td>\n",
       "      <td>[2.4651163, 4.581395, 5.023256, 1.3953488, 3.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>obj365_train_000000634686_000.jpg</td>\n",
       "      <td>[0.58380586, 0.7148745, 0.8758831, 0.69453996,...</td>\n",
       "      <td>[1.8837209, 4.2619047, 3.3953488, 1.6046512, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>COCO_val2014_000000509192.jpg</td>\n",
       "      <td>[0.17919587, 0.6266786, 0.23679179, 0.39790532...</td>\n",
       "      <td>[3.511628, 4.5116277, 3.5581396, 6.0465117, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>COCO_train2014_000000141543.jpg</td>\n",
       "      <td>[0.42648298, 0.29078805, 0.2168133, 0.6667278,...</td>\n",
       "      <td>[3.5238094, 4.2619047, 3.2142856, 2.3809524, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 imageName  \\\n",
       "0        obj365_train_000000121119_000.jpg   \n",
       "1          COCO_train2014_000000015645.jpg   \n",
       "2      openimages_9375bad4f7fb7f07_000.jpg   \n",
       "3        obj365_train_000000152196_000.jpg   \n",
       "4     obj365_train_000000124687_000_v2.jpg   \n",
       "...                                    ...   \n",
       "1120   openimages_760d6765c8fd2994_000.jpg   \n",
       "1121     obj365_train_000000295170_000.jpg   \n",
       "1122     obj365_train_000000634686_000.jpg   \n",
       "1123         COCO_val2014_000000509192.jpg   \n",
       "1124       COCO_train2014_000000141543.jpg   \n",
       "\n",
       "                                                  input  \\\n",
       "0     [0.92500126, 0.3313901, 0.48801482, 0.3691098,...   \n",
       "1     [0.5208456, 0.74441016, 0.82499117, 0.7367672,...   \n",
       "2     [0.3224957, 0.56463706, 0.54845953, 0.4655692,...   \n",
       "3     [0.41080356, 0.8314114, 0.8082274, 0.56753117,...   \n",
       "4     [0.66848725, 0.5300725, 0.5294587, 0.68622065,...   \n",
       "...                                                 ...   \n",
       "1120  [0.17559308, 0.38829413, 0.54677373, 0.5846894...   \n",
       "1121  [0.63697493, 0.59561455, 0.36056772, 0.3848691...   \n",
       "1122  [0.58380586, 0.7148745, 0.8758831, 0.69453996,...   \n",
       "1123  [0.17919587, 0.6266786, 0.23679179, 0.39790532...   \n",
       "1124  [0.42648298, 0.29078805, 0.2168133, 0.6667278,...   \n",
       "\n",
       "                                                 output  \n",
       "0     [2.1190476, 4.6666665, 4.2380953, 1.4761904, 3...  \n",
       "1     [4.9761906, 5.452381, 5.2619047, 6.404762, 4.7...  \n",
       "2     [5.418605, 4.604651, 3.1860466, 6.395349, 3.90...  \n",
       "3     [5.142857, 4.642857, 3.7380953, 6.6904764, 4.0...  \n",
       "4     [2.139535, 3.6046512, 3.2790697, 1.3023256, 3....  \n",
       "...                                                 ...  \n",
       "1120  [2.4186046, 4.7906976, 3.5581396, 5.4418607, 3...  \n",
       "1121  [2.4651163, 4.581395, 5.023256, 1.3953488, 3.2...  \n",
       "1122  [1.8837209, 4.2619047, 3.3953488, 1.6046512, 4...  \n",
       "1123  [3.511628, 4.5116277, 3.5581396, 6.0465117, 3....  \n",
       "1124  [3.5238094, 4.2619047, 3.2142856, 2.3809524, 4...  \n",
       "\n",
       "[1125 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the labels\n",
    "y = all_data.assign(output=all_data.get([list(all_data.columns)[-1]])).get(['output'])\n",
    "\n",
    "# concatenate all the features together into one list\n",
    "scaled_X['input'] = scaled_X.apply(lambda row: np.array(sum([row.iloc[i] for i in range(len(row))], [])), axis=1)\n",
    "scaled_all = scaled_X.get(['input']).merge(y, left_index=True, right_index=True).reset_index()\n",
    "scaled_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageNames</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(obj365_train_000000121119_000.jpg, COCO_train...</td>\n",
       "      <td>([0.92500126, 0.3313901, 0.48801482, 0.3691098...</td>\n",
       "      <td>([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(obj365_train_000000121119_000.jpg, openimages...</td>\n",
       "      <td>([0.92500126, 0.3313901, 0.48801482, 0.3691098...</td>\n",
       "      <td>([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(obj365_train_000000121119_000.jpg, obj365_tra...</td>\n",
       "      <td>([0.92500126, 0.3313901, 0.48801482, 0.3691098...</td>\n",
       "      <td>([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(obj365_train_000000121119_000.jpg, obj365_tra...</td>\n",
       "      <td>([0.92500126, 0.3313901, 0.48801482, 0.3691098...</td>\n",
       "      <td>([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(obj365_train_000000121119_000.jpg, COCO_train...</td>\n",
       "      <td>([0.92500126, 0.3313901, 0.48801482, 0.3691098...</td>\n",
       "      <td>([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632245</th>\n",
       "      <td>(obj365_train_000000295170_000.jpg, COCO_val20...</td>\n",
       "      <td>([0.63697493, 0.59561455, 0.36056772, 0.384869...</td>\n",
       "      <td>([2.4651163, 4.581395, 5.023256, 1.3953488, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632246</th>\n",
       "      <td>(obj365_train_000000295170_000.jpg, COCO_train...</td>\n",
       "      <td>([0.63697493, 0.59561455, 0.36056772, 0.384869...</td>\n",
       "      <td>([2.4651163, 4.581395, 5.023256, 1.3953488, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632247</th>\n",
       "      <td>(obj365_train_000000634686_000.jpg, COCO_val20...</td>\n",
       "      <td>([0.58380586, 0.7148745, 0.8758831, 0.69453996...</td>\n",
       "      <td>([1.8837209, 4.2619047, 3.3953488, 1.6046512, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632248</th>\n",
       "      <td>(obj365_train_000000634686_000.jpg, COCO_train...</td>\n",
       "      <td>([0.58380586, 0.7148745, 0.8758831, 0.69453996...</td>\n",
       "      <td>([1.8837209, 4.2619047, 3.3953488, 1.6046512, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632249</th>\n",
       "      <td>(COCO_val2014_000000509192.jpg, COCO_train2014...</td>\n",
       "      <td>([0.17919587, 0.6266786, 0.23679179, 0.3979053...</td>\n",
       "      <td>([3.511628, 4.5116277, 3.5581396, 6.0465117, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>632250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               imageNames  \\\n",
       "0       (obj365_train_000000121119_000.jpg, COCO_train...   \n",
       "1       (obj365_train_000000121119_000.jpg, openimages...   \n",
       "2       (obj365_train_000000121119_000.jpg, obj365_tra...   \n",
       "3       (obj365_train_000000121119_000.jpg, obj365_tra...   \n",
       "4       (obj365_train_000000121119_000.jpg, COCO_train...   \n",
       "...                                                   ...   \n",
       "632245  (obj365_train_000000295170_000.jpg, COCO_val20...   \n",
       "632246  (obj365_train_000000295170_000.jpg, COCO_train...   \n",
       "632247  (obj365_train_000000634686_000.jpg, COCO_val20...   \n",
       "632248  (obj365_train_000000634686_000.jpg, COCO_train...   \n",
       "632249  (COCO_val2014_000000509192.jpg, COCO_train2014...   \n",
       "\n",
       "                                                    input  \\\n",
       "0       ([0.92500126, 0.3313901, 0.48801482, 0.3691098...   \n",
       "1       ([0.92500126, 0.3313901, 0.48801482, 0.3691098...   \n",
       "2       ([0.92500126, 0.3313901, 0.48801482, 0.3691098...   \n",
       "3       ([0.92500126, 0.3313901, 0.48801482, 0.3691098...   \n",
       "4       ([0.92500126, 0.3313901, 0.48801482, 0.3691098...   \n",
       "...                                                   ...   \n",
       "632245  ([0.63697493, 0.59561455, 0.36056772, 0.384869...   \n",
       "632246  ([0.63697493, 0.59561455, 0.36056772, 0.384869...   \n",
       "632247  ([0.58380586, 0.7148745, 0.8758831, 0.69453996...   \n",
       "632248  ([0.58380586, 0.7148745, 0.8758831, 0.69453996...   \n",
       "632249  ([0.17919587, 0.6266786, 0.23679179, 0.3979053...   \n",
       "\n",
       "                                                   output  \n",
       "0       ([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...  \n",
       "1       ([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...  \n",
       "2       ([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...  \n",
       "3       ([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...  \n",
       "4       ([2.1190476, 4.6666665, 4.2380953, 1.4761904, ...  \n",
       "...                                                   ...  \n",
       "632245  ([2.4651163, 4.581395, 5.023256, 1.3953488, 3....  \n",
       "632246  ([2.4651163, 4.581395, 5.023256, 1.3953488, 3....  \n",
       "632247  ([1.8837209, 4.2619047, 3.3953488, 1.6046512, ...  \n",
       "632248  ([1.8837209, 4.2619047, 3.3953488, 1.6046512, ...  \n",
       "632249  ([3.511628, 4.5116277, 3.5581396, 6.0465117, 3...  \n",
       "\n",
       "[632250 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a list to store the pairs\n",
    "pairs = []\n",
    "\n",
    "# Iterate through each possible pair of rows\n",
    "for i in range(len(scaled_all)):\n",
    "    for j in range(i + 1, len(scaled_all)):\n",
    "        row1 = scaled_all.iloc[i]\n",
    "        row2 = scaled_all.iloc[j]\n",
    "        pairs.append({\n",
    "            'imageNames': (row1['imageName'],row2['imageName']),\n",
    "            'input': (row1['input'],row2['input']),\n",
    "            'output': (row1['output'],row2['output'])\n",
    "        })\n",
    "\n",
    "# Convert the list of pairs to a dataframe\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "pairs_df.to_csv('../data/all_image_pairs.csv')\n",
    "\n",
    "# Display the new dataframe\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the training and testing set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # concatenate all the features together into one list\n",
    "# X_train['concatenated'] = X_train.apply(lambda row: np.array(sum([row.iloc[i] for i in range(len(row))], [])), axis=1)\n",
    "# X_val['concatenated'] = X_val.apply(lambda row: np.array(sum([row.iloc[i] for i in range(len(row))], [])), axis=1)\n",
    "# X_test['concatenated'] = X_test.apply(lambda row: np.array(sum([row.iloc[i] for i in range(len(row))], [])), axis=1)\n",
    "\n",
    "# # get only the concatenated features\n",
    "# X_train = X_train.get('concatenated')\n",
    "# X_val = X_val.get('concatenated')\n",
    "# X_test_label = X_test.get('concatenated')\n",
    "\n",
    "# # get only the ratings\n",
    "# y_train = y_train.get(y_train.columns[0])\n",
    "# y_val = y_val.get(y_val.columns[0])\n",
    "# y_test_label = y_test.get(y_test.columns[0])\n",
    "\n",
    "# # convert everything to float ndarray\n",
    "# X_train = np.vstack(X_train)\n",
    "# X_val = np.vstack(X_val)\n",
    "# X_test = np.vstack(X_test_label)\n",
    "\n",
    "# # convert everything to float ndarray\n",
    "# y_train = np.vstack(y_train)\n",
    "# y_val = np.vstack(y_val)\n",
    "# y_test = np.vstack(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# model definition\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Network Definition\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.shared_net = NeuralNet2(input_size, output_size)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        return self.shared_net(x)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "# Contrastive Loss Definition\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = y_train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/500], Training Loss: 0.2719, Validation loss: 0.23856769502162933\n",
      "Epoch [40/500], Training Loss: 0.2700, Validation loss: 0.29110442847013474\n",
      "Epoch [60/500], Training Loss: 0.2835, Validation loss: 0.29173924028873444\n",
      "Epoch [80/500], Training Loss: 0.2779, Validation loss: 0.28698649629950523\n",
      "Epoch [100/500], Training Loss: 0.2739, Validation loss: 0.2808472290635109\n",
      "Epoch [120/500], Training Loss: 0.2627, Validation loss: 0.26927024871110916\n",
      "Epoch [140/500], Training Loss: 0.2677, Validation loss: 0.3036064878106117\n",
      "Epoch [160/500], Training Loss: 0.2541, Validation loss: 0.2695915587246418\n",
      "Early stopping at epoch 173\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(input_size, output_size).to(device)\n",
    "model.apply(weights_init)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=30)\n",
    "prev_lr = optimizer.param_groups[0]['lr']\n",
    "best_loss = float('inf')  # Initialize to infinity\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stop_patience = 100\n",
    "no_improvement_count = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        X_batch_1, X_batch_2 = X_batch[:X_batch.shape[0]//2], X_batch[X_batch.shape[0]//2:]\n",
    "        X_batch_1 = X_batch_1.to(device)\n",
    "        X_batch_2 = X_batch_2.to(device)\n",
    "        # y_batch_1, y_batch_2 = y_batch[:y_batch.shape[0]//2], y_batch[y_batch.shape[0]//2:]\n",
    "        labels = torch.randint(0, 2, (X_batch.shape[0]//2,)).to(device)  # Example labels (0: different, 1: similar)\n",
    "        \n",
    "        output1, output2 = model(X_batch_1, X_batch_2)\n",
    "        loss = criterion(output1, output2, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    training_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            X_batch_1, X_batch_2 = X_batch[:X_batch.shape[0]//2].to(device), X_batch[X_batch.shape[0]//2:].to(device)\n",
    "            y_batch_1, y_batch_2 = y_batch[:y_batch.shape[0]//2], y_batch[y_batch.shape[0]//2:]\n",
    "            labels = torch.randint(0, 2, (X_batch.shape[0]//2,)).to(device)  # Example labels (0: different, 1: similar)\n",
    "\n",
    "            output1, output2 = model(X_batch_1, X_batch_2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    validation_loss = val_loss / len(val_loader)\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {training_loss:.4f}, Validation loss: {validation_loss}')\n",
    "\n",
    "    # Save the model if it has the best loss so far\n",
    "    if validation_loss < best_loss:\n",
    "        best_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'best_model_siamese.pth')\n",
    "        no_improvement_count = 0  # Reset counter\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(validation_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if no_improvement_count >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "# import numpy as np\n",
    "\n",
    "# # Assume siamese_net is your trained Siamese network\n",
    "# # input1, input2, and pair_labels are your test data and labels\n",
    "\n",
    "# # Forward pass\n",
    "# output1, output2 = siamese_net(input1, input2)\n",
    "\n",
    "# # Calculate Euclidean distances\n",
    "# distances = F.pairwise_distance(output1, output2)\n",
    "\n",
    "# # Determine an optimal threshold\n",
    "# # For simplicity, we can use a fixed threshold (e.g., 0.5), but in practice, you might want to tune this\n",
    "# threshold = 0.5\n",
    "\n",
    "# # Predict labels based on the threshold\n",
    "# predicted_labels = (distances < threshold).float()\n",
    "\n",
    "# # Convert tensors to numpy arrays for metric calculation\n",
    "# pair_labels_np = pair_labels.cpu().numpy()\n",
    "# predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "\n",
    "# # Calculate evaluation metrics\n",
    "# accuracy = accuracy_score(pair_labels_np, predicted_labels_np)\n",
    "# precision = precision_score(pair_labels_np, predicted_labels_np)\n",
    "# recall = recall_score(pair_labels_np, predicted_labels_np)\n",
    "# f1 = f1_score(pair_labels_np, predicted_labels_np)\n",
    "\n",
    "# # Calculate ROC curve and AUC\n",
    "# fpr, tpr, _ = roc_curve(pair_labels_np, distances.cpu().numpy())\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Print metrics\n",
    "# print(f'Accuracy: {accuracy:.4f}')\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')\n",
    "# print(f'F1-Score: {f1:.4f}')\n",
    "# print(f'AUC: {roc_auc:.4f}')\n",
    "\n",
    "# # Plot ROC curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal Loss: 0.01~0.1225"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impactlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
