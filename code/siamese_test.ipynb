{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "img_ext = ['jpg', 'png', 'jpeg']\n",
    "\n",
    "segmentation = ['face', 'body', 'clothing', 'environment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1126"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = []\n",
    "for image in os.listdir(f'{data_dir}/images_ps'):\n",
    "    if image.split('.')[1] in img_ext:\n",
    "        image_list.append(image)\n",
    "\n",
    "len(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframe\n",
    "df = pd.read_csv(f'{data_dir}/annotation_data/face/data_faceIdentity.csv').set_index('imageName')\n",
    "\n",
    "columns = []\n",
    "for i in segmentation:\n",
    "    for file in os.listdir(f'{data_dir}/annotation_data/{i}'):\n",
    "        filename = file.split('.')[0]\n",
    "        columns.append(filename)\n",
    "\n",
    "all_features = pd.DataFrame(index=df.index, columns=columns)\n",
    "# all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put data into initialized dataframe and save it\n",
    "for i in segmentation:\n",
    "    for file in os.listdir(f'{data_dir}/annotation_data/{i}'):\n",
    "        filename = file.split('.')[0]\n",
    "        df = pd.read_csv(f'{data_dir}/annotation_data/{i}/{file}').set_index('imageName')\n",
    "        for j in df.index:\n",
    "            all_features.loc[j, filename] = list(df.loc[j])\n",
    "\n",
    "all_features.to_csv('../data/annotation_data/data_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get the aggregated value of the ratings\n",
    "def get_aggregated_value(filename):\n",
    "    trait_name = filename.split('/')[-1].split('_')[0]\n",
    "    df = pd.read_csv(filename).set_index('imageName')\n",
    "    sum = df.sum(axis=1)\n",
    "    count = df.count(axis=1)\n",
    "    df = pd.DataFrame({'sum':sum, 'count':count})\n",
    "    df[f'{trait_name}'] = df['sum'] / df['count']\n",
    "    df = df.get([f'{trait_name}'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goes through the data for all ratings and add the aggregated value for each image to a list\n",
    "traits = []\n",
    "trait_dir = '../data/rawdata_trait_rating/'\n",
    "for trait_csv in os.listdir('../data/rawdata_trait_rating/'):\n",
    "    traits.append(get_aggregated_value(trait_dir + trait_csv))\n",
    "\n",
    "# create dataframe from the aggregated values of each image and then put them into one column training target\n",
    "traits = pd.concat(traits, axis=1)\n",
    "new_column = str(list(traits.columns))\n",
    "traits[new_column] = traits.apply(lambda row: [\n",
    "    row['attractive'], row['competent'], row['dominant'], row['feminine'], row['open'], row['warm'], row['youthful']\n",
    "], axis=1)\n",
    "traits = traits.get([new_column])\n",
    "# traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the training data and the training target\n",
    "all_data = all_features.merge(traits, left_index=True, right_index=True)\n",
    "\n",
    "# Define a function to convert list elements to floats\n",
    "def convert_list_elements_to_float(cell):\n",
    "    if isinstance(cell, list):\n",
    "        return np.array(cell, dtype=np.float32)\n",
    "    return cell\n",
    "\n",
    "# Apply the function to each cell in the DataFrame\n",
    "all_data = all_data.map(convert_list_elements_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# seperate the training data and the target label\n",
    "X = all_data.get(list(all_data.columns)[:-1])\n",
    "\n",
    "# dataframe to store scaled data\n",
    "scaled_X = pd.DataFrame(index=X.index)\n",
    "\n",
    "# apply Min-Max scaling to training data\n",
    "for i, feature in enumerate(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    stacked = np.vstack(X[feature])\n",
    "    scaled = scaler.fit_transform(stacked)\n",
    "    unstacked = [list(row) for row in scaled]\n",
    "    scaled_X[feature] = unstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    a = np.array(a).reshape(1, -1)\n",
    "    b = np.array(b).reshape(1, -1)\n",
    "    return cosine_similarity(a, b)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageName</th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obj365_train_000000121119_000.jpg</td>\n",
       "      <td>[0.92500126, 0.3313901, 0.48801482, 0.3691098,...</td>\n",
       "      <td>[2.1190476, 4.6666665, 4.2380953, 1.4761904, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_train2014_000000015645.jpg</td>\n",
       "      <td>[0.5208456, 0.74441016, 0.82499117, 0.7367672,...</td>\n",
       "      <td>[4.9761906, 5.452381, 5.2619047, 6.404762, 4.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openimages_9375bad4f7fb7f07_000.jpg</td>\n",
       "      <td>[0.3224957, 0.56463706, 0.54845953, 0.4655692,...</td>\n",
       "      <td>[5.418605, 4.604651, 3.1860466, 6.395349, 3.90...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obj365_train_000000152196_000.jpg</td>\n",
       "      <td>[0.41080356, 0.8314114, 0.8082274, 0.56753117,...</td>\n",
       "      <td>[5.142857, 4.642857, 3.7380953, 6.6904764, 4.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obj365_train_000000124687_000_v2.jpg</td>\n",
       "      <td>[0.66848725, 0.5300725, 0.5294587, 0.68622065,...</td>\n",
       "      <td>[2.139535, 3.6046512, 3.2790697, 1.3023256, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>openimages_760d6765c8fd2994_000.jpg</td>\n",
       "      <td>[0.17559308, 0.38829413, 0.54677373, 0.5846894...</td>\n",
       "      <td>[2.4186046, 4.7906976, 3.5581396, 5.4418607, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>obj365_train_000000295170_000.jpg</td>\n",
       "      <td>[0.63697493, 0.59561455, 0.36056772, 0.3848691...</td>\n",
       "      <td>[2.4651163, 4.581395, 5.023256, 1.3953488, 3.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>obj365_train_000000634686_000.jpg</td>\n",
       "      <td>[0.58380586, 0.7148745, 0.8758831, 0.69453996,...</td>\n",
       "      <td>[1.8837209, 4.2619047, 3.3953488, 1.6046512, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>COCO_val2014_000000509192.jpg</td>\n",
       "      <td>[0.17919587, 0.6266786, 0.23679179, 0.39790532...</td>\n",
       "      <td>[3.511628, 4.5116277, 3.5581396, 6.0465117, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>COCO_train2014_000000141543.jpg</td>\n",
       "      <td>[0.42648298, 0.29078805, 0.2168133, 0.6667278,...</td>\n",
       "      <td>[3.5238094, 4.2619047, 3.2142856, 2.3809524, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 imageName  \\\n",
       "0        obj365_train_000000121119_000.jpg   \n",
       "1          COCO_train2014_000000015645.jpg   \n",
       "2      openimages_9375bad4f7fb7f07_000.jpg   \n",
       "3        obj365_train_000000152196_000.jpg   \n",
       "4     obj365_train_000000124687_000_v2.jpg   \n",
       "...                                    ...   \n",
       "1120   openimages_760d6765c8fd2994_000.jpg   \n",
       "1121     obj365_train_000000295170_000.jpg   \n",
       "1122     obj365_train_000000634686_000.jpg   \n",
       "1123         COCO_val2014_000000509192.jpg   \n",
       "1124       COCO_train2014_000000141543.jpg   \n",
       "\n",
       "                                                  input  \\\n",
       "0     [0.92500126, 0.3313901, 0.48801482, 0.3691098,...   \n",
       "1     [0.5208456, 0.74441016, 0.82499117, 0.7367672,...   \n",
       "2     [0.3224957, 0.56463706, 0.54845953, 0.4655692,...   \n",
       "3     [0.41080356, 0.8314114, 0.8082274, 0.56753117,...   \n",
       "4     [0.66848725, 0.5300725, 0.5294587, 0.68622065,...   \n",
       "...                                                 ...   \n",
       "1120  [0.17559308, 0.38829413, 0.54677373, 0.5846894...   \n",
       "1121  [0.63697493, 0.59561455, 0.36056772, 0.3848691...   \n",
       "1122  [0.58380586, 0.7148745, 0.8758831, 0.69453996,...   \n",
       "1123  [0.17919587, 0.6266786, 0.23679179, 0.39790532...   \n",
       "1124  [0.42648298, 0.29078805, 0.2168133, 0.6667278,...   \n",
       "\n",
       "                                                  label  \n",
       "0     [2.1190476, 4.6666665, 4.2380953, 1.4761904, 3...  \n",
       "1     [4.9761906, 5.452381, 5.2619047, 6.404762, 4.7...  \n",
       "2     [5.418605, 4.604651, 3.1860466, 6.395349, 3.90...  \n",
       "3     [5.142857, 4.642857, 3.7380953, 6.6904764, 4.0...  \n",
       "4     [2.139535, 3.6046512, 3.2790697, 1.3023256, 3....  \n",
       "...                                                 ...  \n",
       "1120  [2.4186046, 4.7906976, 3.5581396, 5.4418607, 3...  \n",
       "1121  [2.4651163, 4.581395, 5.023256, 1.3953488, 3.2...  \n",
       "1122  [1.8837209, 4.2619047, 3.3953488, 1.6046512, 4...  \n",
       "1123  [3.511628, 4.5116277, 3.5581396, 6.0465117, 3....  \n",
       "1124  [3.5238094, 4.2619047, 3.2142856, 2.3809524, 4...  \n",
       "\n",
       "[1125 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the labels\n",
    "y = all_data.assign(label=all_data.get([list(all_data.columns)[-1]])).get(['label'])\n",
    "\n",
    "# concatenate all the features together into one list\n",
    "scaled_X['input'] = scaled_X.apply(lambda row: np.array(sum([row.iloc[i] for i in range(len(row))], [])), axis=1)\n",
    "scaled_all = scaled_X.get(['input']).merge(y, left_index=True, right_index=True).reset_index()\n",
    "scaled_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "pairs = []\n",
    "with open('../data/all_image_pairs.jsonl', 'w') as file:\n",
    "    for i in range(len(scaled_all)):\n",
    "        for j in range(i + 1, len(scaled_all)):\n",
    "            row1 = scaled_all.iloc[i]\n",
    "            row2 = scaled_all.iloc[j]\n",
    "            json_line = json.dumps({\n",
    "                'imageNames': (row1['imageName'],row2['imageName']),\n",
    "                'input': (list(row1['input']),list(row2['input'])),\n",
    "                'label': list(row1['label']-row2['label'])\n",
    "            }, default=convert_numpy)\n",
    "            file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageNames</th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('obj365_train_000000121119_000.jpg', 'COCO_tr...</td>\n",
       "      <td>(array([1.8500025 , 0.6627802 , 0.97602963, .....</td>\n",
       "      <td>[-2.857143   -0.7857146  -1.0238094  -4.928571...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('obj365_train_000000121119_000.jpg', 'openima...</td>\n",
       "      <td>(array([1.8500025 , 0.6627802 , 0.97602963, .....</td>\n",
       "      <td>[-3.2995572   0.06201553  1.0520487  -4.919158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('obj365_train_000000121119_000.jpg', 'obj365_...</td>\n",
       "      <td>(array([1.8500025 , 0.6627802 , 0.97602963, .....</td>\n",
       "      <td>[-3.0238094   0.02380943  0.5        -5.214286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('obj365_train_000000121119_000.jpg', 'obj365_...</td>\n",
       "      <td>(array([1.8500025 , 0.6627802 , 0.97602963, .....</td>\n",
       "      <td>[-0.02048731  1.0620153   0.9590256   0.173864...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('obj365_train_000000121119_000.jpg', 'COCO_tr...</td>\n",
       "      <td>(array([1.8500025 , 0.6627802 , 0.97602963, .....</td>\n",
       "      <td>[-1.2619047   0.9523809   0.40476203  0.142857...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632245</th>\n",
       "      <td>('obj365_train_000000295170_000.jpg', 'COCO_va...</td>\n",
       "      <td>(array([1.2739499 , 1.1912291 , 0.72113544, .....</td>\n",
       "      <td>[-1.0465117   0.06976748  1.4651163  -4.651163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632246</th>\n",
       "      <td>('obj365_train_000000295170_000.jpg', 'COCO_tr...</td>\n",
       "      <td>(array([1.2739499 , 1.1912291 , 0.72113544, .....</td>\n",
       "      <td>[-1.0586932   0.31949043  1.8089702  -0.985603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632247</th>\n",
       "      <td>('obj365_train_000000634686_000.jpg', 'COCO_va...</td>\n",
       "      <td>(array([1.1676117 , 1.429749  , 1.7517662 , .....</td>\n",
       "      <td>[-1.627907   -0.24972296 -0.16279078 -4.44186 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632248</th>\n",
       "      <td>('obj365_train_000000634686_000.jpg', 'COCO_tr...</td>\n",
       "      <td>(array([1.1676117 , 1.429749  , 1.7517662 , .....</td>\n",
       "      <td>[-1.6400886   0.          0.18106318 -0.776301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632249</th>\n",
       "      <td>('COCO_val2014_000000509192.jpg', 'COCO_train2...</td>\n",
       "      <td>(array([0.35839173, 1.2533572 , 0.47358358, .....</td>\n",
       "      <td>[-0.01218152  0.24972296  0.34385395  3.665559...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>632250 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               imageNames  \\\n",
       "0       ('obj365_train_000000121119_000.jpg', 'COCO_tr...   \n",
       "1       ('obj365_train_000000121119_000.jpg', 'openima...   \n",
       "2       ('obj365_train_000000121119_000.jpg', 'obj365_...   \n",
       "3       ('obj365_train_000000121119_000.jpg', 'obj365_...   \n",
       "4       ('obj365_train_000000121119_000.jpg', 'COCO_tr...   \n",
       "...                                                   ...   \n",
       "632245  ('obj365_train_000000295170_000.jpg', 'COCO_va...   \n",
       "632246  ('obj365_train_000000295170_000.jpg', 'COCO_tr...   \n",
       "632247  ('obj365_train_000000634686_000.jpg', 'COCO_va...   \n",
       "632248  ('obj365_train_000000634686_000.jpg', 'COCO_tr...   \n",
       "632249  ('COCO_val2014_000000509192.jpg', 'COCO_train2...   \n",
       "\n",
       "                                                    input  \\\n",
       "0       (array([1.8500025 , 0.6627802 , 0.97602963, .....   \n",
       "1       (array([1.8500025 , 0.6627802 , 0.97602963, .....   \n",
       "2       (array([1.8500025 , 0.6627802 , 0.97602963, .....   \n",
       "3       (array([1.8500025 , 0.6627802 , 0.97602963, .....   \n",
       "4       (array([1.8500025 , 0.6627802 , 0.97602963, .....   \n",
       "...                                                   ...   \n",
       "632245  (array([1.2739499 , 1.1912291 , 0.72113544, .....   \n",
       "632246  (array([1.2739499 , 1.1912291 , 0.72113544, .....   \n",
       "632247  (array([1.1676117 , 1.429749  , 1.7517662 , .....   \n",
       "632248  (array([1.1676117 , 1.429749  , 1.7517662 , .....   \n",
       "632249  (array([0.35839173, 1.2533572 , 0.47358358, .....   \n",
       "\n",
       "                                                    label  \n",
       "0       [-2.857143   -0.7857146  -1.0238094  -4.928571...  \n",
       "1       [-3.2995572   0.06201553  1.0520487  -4.919158...  \n",
       "2       [-3.0238094   0.02380943  0.5        -5.214286...  \n",
       "3       [-0.02048731  1.0620153   0.9590256   0.173864...  \n",
       "4       [-1.2619047   0.9523809   0.40476203  0.142857...  \n",
       "...                                                   ...  \n",
       "632245  [-1.0465117   0.06976748  1.4651163  -4.651163...  \n",
       "632246  [-1.0586932   0.31949043  1.8089702  -0.985603...  \n",
       "632247  [-1.627907   -0.24972296 -0.16279078 -4.44186 ...  \n",
       "632248  [-1.6400886   0.          0.18106318 -0.776301...  \n",
       "632249  [-0.01218152  0.24972296  0.34385395  3.665559...  \n",
       "\n",
       "[632250 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df = pd.read_json('../data/all_image_pairs.jsonl', line=True)\n",
    "# pairs_df = pairs_df.drop(columns=pairs_df.columns[0])\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 30\u001b[0m\n\u001b[0;32m      3\u001b[0m X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_test, y_test, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# # get only the concatenated features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# X_train = X_train.get('concatenated')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# X_val = X_val.get('concatenated')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# X_val = X_val.get(['features'])\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# X_test = X_test.get(['features'])\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloat32\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfloat32\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# X_train.iloc[0].loc['input']\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28meval\u001b[39m(X_train\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m], {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray})\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.dtype'"
     ]
    }
   ],
   "source": [
    "# split the training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(pairs_df.get(['input']), pairs_df.get(['label']), test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# # get only the concatenated features\n",
    "# X_train = X_train.get('concatenated')\n",
    "# X_val = X_val.get('concatenated')\n",
    "# X_test_label = X_test.get('concatenated')\n",
    "\n",
    "# # get only the ratings\n",
    "# y_train = y_train.get(y_train.columns[0])\n",
    "# y_val = y_val.get(y_val.columns[0])\n",
    "# y_test_label = y_test.get(y_test.columns[0])\n",
    "\n",
    "# # convert everything to float ndarray\n",
    "# X_train = np.vstack(X_train)\n",
    "# X_val = np.vstack(X_val)\n",
    "# X_test = np.vstack(X_test_label)\n",
    "\n",
    "# # convert everything to float ndarray\n",
    "# y_train = np.vstack(y_train)\n",
    "# y_val = np.vstack(y_val)\n",
    "# y_test = np.vstack(y_test_label)\n",
    "\n",
    "# get only the concatenated features\n",
    "# X_train = X_train.get(['features'])\n",
    "# X_val = X_val.get(['features'])\n",
    "# X_test = X_test.get(['features'])\n",
    "\n",
    "X_train.iloc[0].loc['input']\n",
    "# # get only the ratings\n",
    "# y_train = y_train.get([y_train.columns[0]])\n",
    "# y_val = y_val.get([y_val.columns[0]])\n",
    "# y_test = y_test.get([y_test.columns[0]])\n",
    "\n",
    "# train = X_train.merge(y_train, left_index=True, right_index=True)\n",
    "# val = X_val.merge(y_val, left_index=True, right_index=True)\n",
    "# test = X_test.merge(y_test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# model definition\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Network Definition\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.shared_net = NeuralNet2(input_size, output_size)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        return self.shared_net(x)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "# Contrastive Loss Definition\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = y_train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/500], Training Loss: 0.2719, Validation loss: 0.23856769502162933\n",
      "Epoch [40/500], Training Loss: 0.2700, Validation loss: 0.29110442847013474\n",
      "Epoch [60/500], Training Loss: 0.2835, Validation loss: 0.29173924028873444\n",
      "Epoch [80/500], Training Loss: 0.2779, Validation loss: 0.28698649629950523\n",
      "Epoch [100/500], Training Loss: 0.2739, Validation loss: 0.2808472290635109\n",
      "Epoch [120/500], Training Loss: 0.2627, Validation loss: 0.26927024871110916\n",
      "Epoch [140/500], Training Loss: 0.2677, Validation loss: 0.3036064878106117\n",
      "Epoch [160/500], Training Loss: 0.2541, Validation loss: 0.2695915587246418\n",
      "Early stopping at epoch 173\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(input_size, output_size).to(device)\n",
    "model.apply(weights_init)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=30)\n",
    "prev_lr = optimizer.param_groups[0]['lr']\n",
    "best_loss = float('inf')  # Initialize to infinity\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stop_patience = 100\n",
    "no_improvement_count = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        X_batch_1, X_batch_2 = X_batch[:X_batch.shape[0]//2], X_batch[X_batch.shape[0]//2:]\n",
    "        X_batch_1 = X_batch_1.to(device)\n",
    "        X_batch_2 = X_batch_2.to(device)\n",
    "        # y_batch_1, y_batch_2 = y_batch[:y_batch.shape[0]//2], y_batch[y_batch.shape[0]//2:]\n",
    "        labels = torch.randint(0, 2, (X_batch.shape[0]//2,)).to(device)  # Example labels (0: different, 1: similar)\n",
    "        \n",
    "        output1, output2 = model(X_batch_1, X_batch_2)\n",
    "        loss = criterion(output1, output2, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    training_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            X_batch_1, X_batch_2 = X_batch[:X_batch.shape[0]//2].to(device), X_batch[X_batch.shape[0]//2:].to(device)\n",
    "            y_batch_1, y_batch_2 = y_batch[:y_batch.shape[0]//2], y_batch[y_batch.shape[0]//2:]\n",
    "            labels = torch.randint(0, 2, (X_batch.shape[0]//2,)).to(device)  # Example labels (0: different, 1: similar)\n",
    "\n",
    "            output1, output2 = model(X_batch_1, X_batch_2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    validation_loss = val_loss / len(val_loader)\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {training_loss:.4f}, Validation loss: {validation_loss}')\n",
    "\n",
    "    # Save the model if it has the best loss so far\n",
    "    if validation_loss < best_loss:\n",
    "        best_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'best_model_siamese.pth')\n",
    "        no_improvement_count = 0  # Reset counter\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(validation_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if no_improvement_count >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "# import numpy as np\n",
    "\n",
    "# # Assume siamese_net is your trained Siamese network\n",
    "# # input1, input2, and pair_labels are your test data and labels\n",
    "\n",
    "# # Forward pass\n",
    "# output1, output2 = siamese_net(input1, input2)\n",
    "\n",
    "# # Calculate Euclidean distances\n",
    "# distances = F.pairwise_distance(output1, output2)\n",
    "\n",
    "# # Determine an optimal threshold\n",
    "# # For simplicity, we can use a fixed threshold (e.g., 0.5), but in practice, you might want to tune this\n",
    "# threshold = 0.5\n",
    "\n",
    "# # Predict labels based on the threshold\n",
    "# predicted_labels = (distances < threshold).float()\n",
    "\n",
    "# # Convert tensors to numpy arrays for metric calculation\n",
    "# pair_labels_np = pair_labels.cpu().numpy()\n",
    "# predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "\n",
    "# # Calculate evaluation metrics\n",
    "# accuracy = accuracy_score(pair_labels_np, predicted_labels_np)\n",
    "# precision = precision_score(pair_labels_np, predicted_labels_np)\n",
    "# recall = recall_score(pair_labels_np, predicted_labels_np)\n",
    "# f1 = f1_score(pair_labels_np, predicted_labels_np)\n",
    "\n",
    "# # Calculate ROC curve and AUC\n",
    "# fpr, tpr, _ = roc_curve(pair_labels_np, distances.cpu().numpy())\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Print metrics\n",
    "# print(f'Accuracy: {accuracy:.4f}')\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')\n",
    "# print(f'F1-Score: {f1:.4f}')\n",
    "# print(f'AUC: {roc_auc:.4f}')\n",
    "\n",
    "# # Plot ROC curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal Loss: 0.01~0.1225"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impactlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
